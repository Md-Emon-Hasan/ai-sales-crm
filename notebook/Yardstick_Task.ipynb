{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GN_xXjjqNNQ"
      },
      "source": [
        "# Conversation Management & JSON Schema Extraction (Groq OpenAI-Compatible)\n",
        "# Author: Md. Hasan Imon\n",
        "# Objective:\n",
        "# - Task 1: Conversation management + summarization\n",
        "# - Task 2: JSON schema classification & extraction\n",
        "# Notes:\n",
        "# - Framework-free: only Python + requests + openai client + jsonschema\n",
        "# - Uses Groq API (OpenAI-compatible endpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DAI9J3-6qOLY"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet openai jsonschema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiQRq0RxREZ"
      },
      "source": [
        "\n",
        "## Securely provide your Groq API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7klx-vqxUej",
        "outputId": "0986ecb2-9b1e-462f-a061-cdaa54cab055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Hidden input in Colab\n",
        "GROQ_API_KEY = getpass(\"Enter your Groq API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omEtCE7Rqa4C"
      },
      "source": [
        "## Initialize OpenAI-compatible client pointing at Groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq47tusnqYkp",
        "outputId": "aaed3483-75ed-4e1d-8c24-1244a89dbb1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized with model: openai/gpt-oss-20b\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
        "print(\"Groq client initialized with model:\", MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmoWqGV-qibA"
      },
      "source": [
        "\n",
        "## Conversation Manager Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "nVqDRuF8qf7o"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, model_name: str, client, summary_every_k: int = 3):\n",
        "        self.model_name = model_name\n",
        "        self.client = client\n",
        "        self.history = []\n",
        "        self.run_counter = 0\n",
        "        self.summary_every_k = summary_every_k\n",
        "        self.summaries = []\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        assert role in (\"user\",\"assistant\",\"system\")\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def get_last_n_turns(self, n:int):\n",
        "        return self.history[-2*n:] if 2*n <= len(self.history) else self.history[:]\n",
        "\n",
        "    def truncate_by_chars(self, max_chars:int):\n",
        "        kept = []\n",
        "        total = 0\n",
        "        for msg in reversed(self.history):\n",
        "            l = len(msg[\"content\"])\n",
        "            if total + l > max_chars:\n",
        "                break\n",
        "            kept.append(msg)\n",
        "            total += l\n",
        "        self.history = list(reversed(kept))\n",
        "\n",
        "    def truncate_by_words(self, max_words:int):\n",
        "        kept = []\n",
        "        total = 0\n",
        "        for msg in reversed(self.history):\n",
        "            w = len(msg[\"content\"].split())\n",
        "            if total + w > max_words:\n",
        "                break\n",
        "            kept.append(msg)\n",
        "            total += w\n",
        "        self.history = list(reversed(kept))\n",
        "\n",
        "    def summarize_history(self, summary_prompt_extra=\"\"):\n",
        "        conversation_text = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.history])\n",
        "        system_msg = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful summarizer. Produce a concise summary of the conversation.\"\n",
        "        }\n",
        "        user_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Summarize the following conversation into a short, factual summary (6-12 bullet points). {summary_prompt_extra}\\n\\nConversation:\\n{conversation_text}\"\n",
        "        }\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[system_msg, user_msg],\n",
        "            max_tokens=512,\n",
        "            temperature=0.0\n",
        "        )\n",
        "        summary_text = \"\"\n",
        "        if hasattr(resp, \"choices\") and len(resp.choices) > 0:\n",
        "            summary_text = resp.choices[0].message.content # Corrected access\n",
        "        ts = int(time.time())\n",
        "        self.summaries.append({\"timestamp\": ts, \"summary\": summary_text})\n",
        "        self.history = [{\"role\":\"system\", \"content\": f\"[AUTO-SUMMARY at {time.ctime(ts)}]\\n{summary_text}\"}]\n",
        "        return summary_text\n",
        "\n",
        "    def process_user_message(self, user_text: str, do_summarize_if_needed=True):\n",
        "        self.add_message(\"user\", user_text)\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=self.history + [{\"role\":\"user\",\"content\":user_text}],\n",
        "            max_tokens=512,\n",
        "            temperature=0.0\n",
        "        )\n",
        "        assistant_text = \"\"\n",
        "        if hasattr(resp, \"choices\") and len(resp.choices) > 0:\n",
        "            assistant_text = resp.choices[0].message.content # Corrected access\n",
        "        self.add_message(\"assistant\", assistant_text)\n",
        "        self.run_counter += 1\n",
        "        summary = None\n",
        "        if do_summarize_if_needed and self.summary_every_k > 0 and self.run_counter % self.summary_every_k == 0:\n",
        "            summary = self.summarize_history()\n",
        "        return assistant_text, summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVtbJRCqmAg"
      },
      "source": [
        "## 5) Demonstration of Task 1: multiple conversation samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05504ae8",
        "outputId": "eef5c343-3ecb-4924-e0c4-27fcd5eaae72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Turn 1 ===\n",
            "User: Hi, I want to prepare a project-based portfolio for AI agent jobs. Can you help?\n",
            "Assistant (truncated): \n",
            "\n",
            "=== Turn 2 ===\n",
            "User: I have experience with LangGraph and LangChain. I need advanced project ideas.\n",
            "Assistant (truncated): \n",
            "\n",
            "=== Turn 3 ===\n",
            "User: Suggest a multi-agent architecture for a RAG-based customer support assistant.\n",
            "Assistant (truncated): ## ðŸš€ Multiâ€‘Agent RAGâ€‘Based Customer Support Assistant  \n",
            "*(Designed for LangChain + LangGraph, but agnostic to the underlying LLM provider)*  \n",
            "\n",
            "Below is a **complete, productionâ€‘ready architecture** that splits the customerâ€‘support workflow into a set of cooperating agents. Each agent has a single, wellâ€‘defined responsibility, communicates via a lightweight message bus, and can be swapped, scaled, ...\n",
            "\n",
            "--- AUTO SUMMARY GENERATED ---\n",
            "- User seeks help building a projectâ€‘based portfolio for AI agent roles.  \n",
            "- User has experience with LangGraph and LangChain and wants advanced project ideas.  \n",
            "- User specifically requests a multiâ€‘agent architecture for a Retrievalâ€‘Augmented Generation (RAG) customerâ€‘support assistant.  \n",
            "- Assistant proposes a productionâ€‘ready, LangChain/LangGraphâ€‘agnostic architecture.  \n",
            "- The design splits the workflow into discrete agents: an NLP parser, a retrieval agent, a generation agent, etc., each with a single responsibility.  \n",
            "- Agents communicate via a lightweight message bus and can be swapped, scaled, or upgraded independently.  \n",
            "- The highâ€‘level flow includes a frontâ€‘end/API layer, an orchestrator (graph), and the individual agents (A1, A2, A3, â€¦).  \n",
            "- The assistant outlines the overall ar\n",
            "-----------------------------\n",
            "\n",
            "=== Turn 4 ===\n",
            "User: How to implement fallback logic and tool routing?\n",
            "Assistant (truncated): \n",
            "\n",
            "=== Turn 5 ===\n",
            "User: Which vector DB, and how to store short-term vs long-term memory effectively?\n",
            "Assistant (truncated): \n",
            "\n",
            "=== Turn 6 ===\n",
            "User: Show me a concise plan for deployment and CI/CD for such a multi-agent app.\n",
            "Assistant (truncated): ## ðŸš€ Deployment & CI/CD Blueprint for a Multiâ€‘Agent RAG Assistant\n",
            "\n",
            "| Layer | What | Why | Tooling |\n",
            "|-------|------|-----|---------|\n",
            "| **Source** | Git repo (GitHub/GitLab/Bitbucket) | Version control, collaboration | Git |\n",
            "| **Build** | Docker images per agent + shared runtime | Isolation, reproducible builds | Docker, Dockerâ€‘Compose (dev) |\n",
            "| **Test** | Unit, integration, contract, endâ€‘toâ€‘end | ...\n",
            "\n",
            "--- AUTO SUMMARY GENERATED ---\n",
            "- **Goal:** User wants a productionâ€‘ready, multiâ€‘agent RAG customerâ€‘support assistant using LangChain/LangGraph.  \n",
            "- **Initial Architecture:** Assistant proposes a modular design with discrete agents (NLP parser, retrieval, generation, etc.) communicating over a lightweight message bus, all orchestrated by a graph.  \n",
            "- **User Followâ€‘ups:**  \n",
            "  - Asked how to implement fallback logic and tool routing.  \n",
            "  - Asked which vector database to use and how to manage shortâ€‘term vs longâ€‘term memory.  \n",
            "  - Requested a concise deployment and CI/CD plan.  \n",
            "- **Assistant Response (Deployment & CI/CD):**  \n",
            "  - Outlined a layered pipeline: source control â†’ Docker builds â†’ unit/integration tests â†’ container registry â†’ Kubernetes orchestration (Helm/Kustomize).  \n",
            "  - Included tooling for secrets (Vault/KMS)\n",
            "-----------------------------\n",
            "\n",
            "=== Current History Stored ===\n",
            "SYSTEM: [AUTO-SUMMARY at Sat Sep 13 16:03:32 2025]\n",
            "- **Goal:** User wants a productionâ€‘ready, multiâ€‘agent RAG customerâ€‘support assistant using LangChain/LangGraph.  \n",
            "- **Initial Architecture:** Assistant proposes a modular design with discrete agents (NLP parser, retrieval, generation, etc.) communicating over a lightweight message bus, all orchestrated by a graph.  \n",
            "- **User Followâ€‘ups:**  \n",
            "  - Asked how...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cm = ConversationManager(model_name=MODEL_NAME, client=client, summary_every_k=3)\n",
        "\n",
        "samples = [\n",
        "    \"Hi, I want to prepare a project-based portfolio for AI agent jobs. Can you help?\",\n",
        "    \"I have experience with LangGraph and LangChain. I need advanced project ideas.\",\n",
        "    \"Suggest a multi-agent architecture for a RAG-based customer support assistant.\",\n",
        "    \"How to implement fallback logic and tool routing?\",\n",
        "    \"Which vector DB, and how to store short-term vs long-term memory effectively?\",\n",
        "    \"Show me a concise plan for deployment and CI/CD for such a multi-agent app.\"\n",
        "]\n",
        "\n",
        "for i, s in enumerate(samples, start=1):\n",
        "    assistant_text, summary = cm.process_user_message(s)\n",
        "    print(f\"\\n=== Turn {i} ===\")\n",
        "    print(\"User:\", s)\n",
        "    print(\"Assistant (truncated):\", (assistant_text[:400] + \"...\") if len(assistant_text)>400 else assistant_text)\n",
        "    if summary:\n",
        "        print(\"\\n--- AUTO SUMMARY GENERATED ---\")\n",
        "        print(summary[:800])\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "print(\"\\n=== Current History Stored ===\")\n",
        "for m in cm.history:\n",
        "    print(f\"{m['role'].upper()}: {m['content'][:400]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmg48991q7fv"
      },
      "source": [
        "### Demonstrate truncation options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCicrMAjq8On",
        "outputId": "0ce6d7dd-869c-46b9-d663-447d5ecc0b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-- Last 2 turns --\n",
            "{'role': 'user', 'content': 'User question #7 â€” details about task 7.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #7 â€” answers and clarifications.'}\n",
            "{'role': 'user', 'content': 'User question #8 â€” details about task 8.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #8 â€” answers and clarifications.'}\n",
            "\n",
            "-- Truncate by chars = 200 --\n",
            "{'role': 'user', 'content': 'User question #7 â€” details about task 7.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #7 â€” answers and clarifications.'}\n",
            "{'role': 'user', 'content': 'User question #8 â€” details about task 8.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #8 â€” answers and clarifications.'}\n",
            "\n",
            "-- Truncate by words = 40 --\n",
            "{'role': 'assistant', 'content': 'Assistant reply #6 â€” answers and clarifications.'}\n",
            "{'role': 'user', 'content': 'User question #7 â€” details about task 7.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #7 â€” answers and clarifications.'}\n",
            "{'role': 'user', 'content': 'User question #8 â€” details about task 8.'}\n",
            "{'role': 'assistant', 'content': 'Assistant reply #8 â€” answers and clarifications.'}\n"
          ]
        }
      ],
      "source": [
        "# Re-populate some history\n",
        "cm.history = []\n",
        "for i in range(1,9):\n",
        "    cm.add_message(\"user\", f\"User question #{i} â€” details about task {i}.\")\n",
        "    cm.add_message(\"assistant\", f\"Assistant reply #{i} â€” answers and clarifications.\")\n",
        "\n",
        "print(\"\\n-- Last 2 turns --\")\n",
        "for m in cm.get_last_n_turns(2):\n",
        "    print(m)\n",
        "\n",
        "print(\"\\n-- Truncate by chars = 200 --\")\n",
        "cm.truncate_by_chars(200)\n",
        "for m in cm.history:\n",
        "    print(m)\n",
        "\n",
        "cm.history = []\n",
        "for i in range(1,9):\n",
        "    cm.add_message(\"user\", f\"User question #{i} â€” details about task {i}.\")\n",
        "    cm.add_message(\"assistant\", f\"Assistant reply #{i} â€” answers and clarifications.\")\n",
        "\n",
        "print(\"\\n-- Truncate by words = 40 --\")\n",
        "cm.truncate_by_words(40)\n",
        "for m in cm.history:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OPQpKXOrAWY"
      },
      "source": [
        "## Task 2: JSON Schema Classification & Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-KB-OWIBrBgo"
      },
      "outputs": [],
      "source": [
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
        "        \"phone\": {\"type\": \"string\"},\n",
        "        \"location\": {\"type\": \"string\"},\n",
        "        \"age\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 150}\n",
        "    },\n",
        "    \"required\": [\"name\"],\n",
        "    \"additionalProperties\": False\n",
        "}\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"extract_user_info\",\n",
        "        \"description\": \"Extract contact and basic personal info from the conversation or message.\",\n",
        "        \"parameters\": schema\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73YRikoSrHaa"
      },
      "source": [
        "### Helper: function-calling model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "_Rx63lU7rIhx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def call_function_calling_model(client, model_name, user_message, functions):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are a JSON extractor. Call the provided function with structured JSON when possible.\"},\n",
        "        {\"role\":\"user\",\"content\":user_message}\n",
        "    ]\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "        max_tokens=512,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    if hasattr(resp, \"choices\") and len(resp.choices) > 0:\n",
        "        msg = resp.choices[0].message\n",
        "        if hasattr(msg, \"function_call\") and msg.function_call:\n",
        "            fname = msg.function_call.name # Corrected access\n",
        "            fargs_raw = msg.function_call.arguments # Corrected access\n",
        "            try:\n",
        "                fargs = json.loads(fargs_raw)\n",
        "            except Exception:\n",
        "                json_text_match = re.search(r\"(\\{.*\\})\", fargs_raw, re.S)\n",
        "                if json_text_match:\n",
        "                    fargs = json.loads(json_text_match.group(1))\n",
        "                else:\n",
        "                    fargs = None\n",
        "            return fname, fargs, resp\n",
        "    if hasattr(resp, \"choices\") and len(resp.choices) > 0:\n",
        "        txt = resp.choices[0].message.content # Corrected access\n",
        "        json_text_match = re.search(r\"(\\{.*\\})\", txt, re.S)\n",
        "        if json_text_match:\n",
        "            try:\n",
        "                parsed = json.loads(json_text_match.group(1))\n",
        "                return None, parsed, resp\n",
        "            except:\n",
        "                pass\n",
        "    return None, None, resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcb-E3RLrMXv"
      },
      "source": [
        "\n",
        "### Sample chats for extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9HmDGIGrNk3",
        "outputId": "ae6727ba-d0f9-48c4-c064-9e2a14c36567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "Input: Hello, I'm Md. Hasan Imon. You can reach me at emon.mlengineer@gmail.com. My phone is +8801834363533. I live in Savar, Dhaka. I'm 24 years old.\n",
            "Function called: extract_user_info\n",
            "Parsed args: {'age': 24, 'email': 'emon.mlengineer@gmail.com', 'location': 'Savar, Dhaka', 'name': 'Md. Hasan Imon', 'phone': '+8801834363533'}\n",
            "Validation: OK\n",
            "\n",
            "--- Sample 2 ---\n",
            "Input: Hey there â€” name's Emon. I'm 25 and currently living near Dhaka city. Email: md.emon.hasan@example.com. Call me maybe 01834363533\n",
            "Function called: extract_user_info\n",
            "Parsed args: {'age': 25, 'email': 'md.emon.hasan@example.com', 'location': 'Dhaka city', 'name': 'Emon', 'phone': '01834363533'}\n",
            "Validation: OK\n",
            "\n",
            "--- Sample 3 ---\n",
            "Input: Hi, this is Ayesha from Chittagong. I'm in my early 30s. My email is ayesha.work@mailprovider.com. Don't have a phone right now.\n",
            "Function called: extract_user_info\n",
            "Parsed args: {'age': 30, 'email': 'ayesha.work@mailprovider.com', 'location': 'Chittagong', 'name': 'Ayesha'}\n",
            "Validation: OK\n"
          ]
        }
      ],
      "source": [
        "sample_chats = [\n",
        "    \"Hello, I'm Md. Hasan Imon. You can reach me at emon.mlengineer@gmail.com. My phone is +8801834363533. I live in Savar, Dhaka. I'm 24 years old.\",\n",
        "    \"Hey there â€” name's Emon. I'm 25 and currently living near Dhaka city. Email: md.emon.hasan@example.com. Call me maybe 01834363533\",\n",
        "    \"Hi, this is Ayesha from Chittagong. I'm in my early 30s. My email is ayesha.work@mailprovider.com. Don't have a phone right now.\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for i, chat in enumerate(sample_chats, start=1):\n",
        "    fname, fargs, raw = call_function_calling_model(client, MODEL_NAME, chat, functions)\n",
        "    print(f\"\\n--- Sample {i} ---\")\n",
        "    print(\"Input:\", chat)\n",
        "    print(\"Function called:\", fname)\n",
        "    print(\"Parsed args:\", fargs)\n",
        "    valid = False\n",
        "    errors = None\n",
        "    if fargs:\n",
        "        try:\n",
        "            validate(instance=fargs, schema=schema)\n",
        "            valid = True\n",
        "        except ValidationError as e:\n",
        "            errors = str(e)\n",
        "    print(\"Validation:\", \"OK\" if valid else f\"FAILED: {errors}\")\n",
        "    results.append({\"input\":chat, \"function\":fname, \"parsed\":fargs, \"valid\":valid, \"errors\":errors})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
